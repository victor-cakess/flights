{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eba0eca-053d-4fc9-bdaa-414f67789847",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This script implements an end-to-end data transformation and consolidation pipeline for flight data stored in CSV files. It addresses issues such as inconsistent data types, duplicate rows, and formatting irregularities, ultimately unifying the data into a single, reliable table in a DuckDB database.\n",
    "\n",
    "## What it does\n",
    "\n",
    "- **Database Setup & CSV Import:**  \n",
    "  Connects to (or creates) a DuckDB database file and imports CSV files from the \"helium\" folder. Each CSV file is converted into a table with its name derived from the filename.\n",
    "\n",
    "- **Data Consolidation:**  \n",
    "  Merges individual CSV tables into a single \"flights\" table using a UNION ALL query, making the dataset easier to query and analyze.\n",
    "\n",
    "- **Deduplication & Data Cleaning:**  \n",
    "  Identifies and removes duplicate rows to ensure data integrity. It also cleans string columns by removing extraneous characters and standardizing empty values.\n",
    "\n",
    "- **Schema Transformation:**  \n",
    "  Updates the schema by converting certain columns to appropriate data types (e.g., from VARCHAR to INT) and dropping unnecessary columns. This step standardizes the structure of the data.\n",
    "\n",
    "## Why it does it\n",
    "\n",
    "- **Consistency:**  \n",
    "  The script handles inconsistencies in the data by enforcing uniform data types and standardized formatting, which is essential for reliable analysis.\n",
    "\n",
    "- **Efficiency:**  \n",
    "  Automating the import, cleaning, and consolidation processes reduces the need for manual intervention and minimizes the risk of human error.\n",
    "\n",
    "- **Data Integrity:**  \n",
    "  Deduplication and careful data cleaning ensure that the final consolidated dataset is accurate and ready for further analysis.\n",
    "\n",
    "- **Scalability:**  \n",
    "  By merging multiple CSV sources into one unified table, the pipeline simplifies data management and makes the dataset easier to query at scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea193db-bd1c-4a97-8348-68549334cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a1098-a493-41a0-a4f4-f35b63a5b54a",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This script creates, if it doesn't exist, a DuckDB database file named \"flight_data.duckdb\" and automatically imports all CSV files from the \"helium\" folder. Each CSV file is turned into a table in the database, with the table name derived from the CSV filename.\n",
    "\n",
    "## What it does\n",
    "\n",
    "- **Database connection:**  \n",
    "  Connects to the DuckDB database file, creating it if necessary, so you don't have to worry about setting up the database manually.\n",
    "\n",
    "- **CSV file discovery:**  \n",
    "  Uses glob to find all CSV files in the \"helium\" folder, ensuring every file is on deck for import.\n",
    "\n",
    "- **Table creation:**  \n",
    "  For each CSV file, it:\n",
    "  - Extracts the filename (without the extension) to use as the table name.\n",
    "  - Reads the CSV file with `read_csv_auto`, forcing all values to be treated as strings.\n",
    "  - Creates a new table in DuckDB based on the CSV content.\n",
    "\n",
    "- **Progress Feedback:**  \n",
    "  Prints a confirmation message for each table created, so you're never left in the dark about what's been processed.\n",
    "\n",
    "- **Cleanup:**  \n",
    "  Closes the database connection once all files have been imported, keeping everything neat and tidy.\n",
    "\n",
    "## Why it does it\n",
    "\n",
    "Because manually importing CSV files into a database is for those who enjoy unnecessary hassle. This script automates the import process, ensuring your flight data is consistently and reliably loaded into DuckDB, freeing you up to focus on listening to Alisson Becker's chant over and over again because he saved Liverpool against PSG, let's be honest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e73dabc-b663-4ab8-97b7-19dcecf8483f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'VRA_2002_02' created from 'helium/VRA_2002_02.csv'.\n",
      "Table 'VRA_2006_03' created from 'helium/VRA_2006_03.csv'.\n",
      "Table 'VRA_2001_12' created from 'helium/VRA_2001_12.csv'.\n",
      "Table 'VRA_2023_02' created from 'helium/VRA_2023_02.csv'.\n",
      "Table 'VRA_2004_11' created from 'helium/VRA_2004_11.csv'.\n",
      "Table 'VRA_2008_06' created from 'helium/VRA_2008_06.csv'.\n",
      "Table 'VRA_2004_05' created from 'helium/VRA_2004_05.csv'.\n",
      "Table 'VRA_2010_06' created from 'helium/VRA_2010_06.csv'.\n",
      "Table 'VRA_2024_04' created from 'helium/VRA_2024_04.csv'.\n",
      "Table 'VRA_2014_09' created from 'helium/VRA_2014_09.csv'.\n",
      "Table 'VRA_2006_05' created from 'helium/VRA_2006_05.csv'.\n",
      "Table 'VRA_2016_10' created from 'helium/VRA_2016_10.csv'.\n",
      "Table 'VRA_2007_02' created from 'helium/VRA_2007_02.csv'.\n",
      "Table 'VRA_2022_04' created from 'helium/VRA_2022_04.csv'.\n",
      "Table 'VRA_2006_11' created from 'helium/VRA_2006_11.csv'.\n",
      "Table 'VRA_2020_02' created from 'helium/VRA_2020_02.csv'.\n",
      "Table 'VRA_2009_01' created from 'helium/VRA_2009_01.csv'.\n",
      "Table 'VRA_2010_02' created from 'helium/VRA_2010_02.csv'.\n",
      "Table 'VRA_2014_05' created from 'helium/VRA_2014_05.csv'.\n",
      "Table 'VRA_2006_02' created from 'helium/VRA_2006_02.csv'.\n",
      "Table 'VRA_2005_04' created from 'helium/VRA_2005_04.csv'.\n",
      "Table 'VRA_2017_10' created from 'helium/VRA_2017_10.csv'.\n",
      "Table 'VRA_2020_11' created from 'helium/VRA_2020_11.csv'.\n",
      "Table 'VRA_2018_02' created from 'helium/VRA_2018_02.csv'.\n",
      "Table 'VRA_2014_03' created from 'helium/VRA_2014_03.csv'.\n",
      "Table 'VRA_2017_09' created from 'helium/VRA_2017_09.csv'.\n",
      "Table 'VRA_2003_09' created from 'helium/VRA_2003_09.csv'.\n",
      "Table 'VRA_2021_10' created from 'helium/VRA_2021_10.csv'.\n",
      "Table 'VRA_2017_12' created from 'helium/VRA_2017_12.csv'.\n",
      "Table 'VRA_2018_12' created from 'helium/VRA_2018_12.csv'.\n",
      "Table 'VRA_2011_03' created from 'helium/VRA_2011_03.csv'.\n",
      "Table 'VRA_2013_04' created from 'helium/VRA_2013_04.csv'.\n",
      "Table 'VRA_2013_07' created from 'helium/VRA_2013_07.csv'.\n",
      "Table 'VRA_2017_11' created from 'helium/VRA_2017_11.csv'.\n",
      "Table 'VRA_2019_05' created from 'helium/VRA_2019_05.csv'.\n",
      "Table 'VRA_2011_04' created from 'helium/VRA_2011_04.csv'.\n",
      "Table 'VRA_2012_03' created from 'helium/VRA_2012_03.csv'.\n",
      "Table 'VRA_2013_10' created from 'helium/VRA_2013_10.csv'.\n",
      "Table 'VRA_2015_11' created from 'helium/VRA_2015_11.csv'.\n",
      "Table 'VRA_2011_08' created from 'helium/VRA_2011_08.csv'.\n",
      "Table 'VRA_2019_01' created from 'helium/VRA_2019_01.csv'.\n",
      "Table 'VRA_2023_10' created from 'helium/VRA_2023_10.csv'.\n",
      "Table 'VRA_2015_07' created from 'helium/VRA_2015_07.csv'.\n",
      "Table 'VRA_2003_07' created from 'helium/VRA_2003_07.csv'.\n",
      "Table 'VRA_2005_02' created from 'helium/VRA_2005_02.csv'.\n",
      "Table 'VRA_2017_04' created from 'helium/VRA_2017_04.csv'.\n",
      "Table 'VRA_2022_07' created from 'helium/VRA_2022_07.csv'.\n",
      "Table 'VRA_2004_08' created from 'helium/VRA_2004_08.csv'.\n",
      "Table 'VRA_2017_07' created from 'helium/VRA_2017_07.csv'.\n",
      "Table 'VRA_2004_04' created from 'helium/VRA_2004_04.csv'.\n",
      "Table 'VRA_2020_12' created from 'helium/VRA_2020_12.csv'.\n",
      "Table 'VRA_2007_11' created from 'helium/VRA_2007_11.csv'.\n",
      "Table 'VRA_2020_03' created from 'helium/VRA_2020_03.csv'.\n",
      "Table 'VRA_2005_12' created from 'helium/VRA_2005_12.csv'.\n",
      "Table 'VRA_2003_01' created from 'helium/VRA_2003_01.csv'.\n",
      "Table 'VRA_2002_03' created from 'helium/VRA_2002_03.csv'.\n",
      "Table 'VRA_2024_05' created from 'helium/VRA_2024_05.csv'.\n",
      "Table 'VRA_2017_03' created from 'helium/VRA_2017_03.csv'.\n",
      "Table 'VRA_2006_10' created from 'helium/VRA_2006_10.csv'.\n",
      "Table 'VRA_2010_03' created from 'helium/VRA_2010_03.csv'.\n",
      "Table 'VRA_2018_04' created from 'helium/VRA_2018_04.csv'.\n",
      "Table 'VRA_2012_08' created from 'helium/VRA_2012_08.csv'.\n",
      "Table 'VRA_2017_01' created from 'helium/VRA_2017_01.csv'.\n",
      "Table 'VRA_2023_03' created from 'helium/VRA_2023_03.csv'.\n",
      "Table 'VRA_2010_04' created from 'helium/VRA_2010_04.csv'.\n",
      "Table 'VRA_2012_01' created from 'helium/VRA_2012_01.csv'.\n",
      "Table 'VRA_2011_02' created from 'helium/VRA_2011_02.csv'.\n",
      "Table 'VRA_2002_01' created from 'helium/VRA_2002_01.csv'.\n",
      "Table 'VRA_2009_08' created from 'helium/VRA_2009_08.csv'.\n",
      "Table 'VRA_2005_03' created from 'helium/VRA_2005_03.csv'.\n",
      "Table 'VRA_2018_03' created from 'helium/VRA_2018_03.csv'.\n",
      "Table 'VRA_2003_04' created from 'helium/VRA_2003_04.csv'.\n",
      "Table 'VRA_2002_09' created from 'helium/VRA_2002_09.csv'.\n",
      "Table 'VRA_2024_12' created from 'helium/VRA_2024_12.csv'.\n",
      "Table 'VRA_2016_08' created from 'helium/VRA_2016_08.csv'.\n",
      "Table 'VRA_2023_04' created from 'helium/VRA_2023_04.csv'.\n",
      "Table 'VRA_2018_05' created from 'helium/VRA_2018_05.csv'.\n",
      "Table 'VRA_2024_01' created from 'helium/VRA_2024_01.csv'.\n",
      "Table 'VRA_2019_03' created from 'helium/VRA_2019_03.csv'.\n",
      "Table 'VRA_2000_04' created from 'helium/VRA_2000_04.csv'.\n",
      "Table 'VRA_2002_11' created from 'helium/VRA_2002_11.csv'.\n",
      "Table 'VRA_2016_06' created from 'helium/VRA_2016_06.csv'.\n",
      "Table 'VRA_2020_08' created from 'helium/VRA_2020_08.csv'.\n",
      "Table 'VRA_2007_07' created from 'helium/VRA_2007_07.csv'.\n",
      "Table 'VRA_2016_05' created from 'helium/VRA_2016_05.csv'.\n",
      "Table 'VRA_2020_10' created from 'helium/VRA_2020_10.csv'.\n",
      "Table 'VRA_2024_11' created from 'helium/VRA_2024_11.csv'.\n",
      "Table 'VRA_2001_06' created from 'helium/VRA_2001_06.csv'.\n",
      "Table 'VRA_2011_11' created from 'helium/VRA_2011_11.csv'.\n",
      "Table 'VRA_2012_05' created from 'helium/VRA_2012_05.csv'.\n",
      "Table 'VRA_2012_07' created from 'helium/VRA_2012_07.csv'.\n",
      "Table 'VRA_2000_10' created from 'helium/VRA_2000_10.csv'.\n",
      "Table 'VRA_2016_01' created from 'helium/VRA_2016_01.csv'.\n",
      "Table 'VRA_2013_11' created from 'helium/VRA_2013_11.csv'.\n",
      "Table 'VRA_2024_03' created from 'helium/VRA_2024_03.csv'.\n",
      "Table 'VRA_2010_11' created from 'helium/VRA_2010_11.csv'.\n",
      "Table 'VRA_2008_03' created from 'helium/VRA_2008_03.csv'.\n",
      "Table 'VRA_2022_08' created from 'helium/VRA_2022_08.csv'.\n",
      "Table 'VRA_2021_04' created from 'helium/VRA_2021_04.csv'.\n",
      "Table 'VRA_2003_12' created from 'helium/VRA_2003_12.csv'.\n",
      "Table 'VRA_2013_06' created from 'helium/VRA_2013_06.csv'.\n",
      "Table 'VRA_2001_01' created from 'helium/VRA_2001_01.csv'.\n",
      "Table 'VRA_2020_01' created from 'helium/VRA_2020_01.csv'.\n",
      "Table 'VRA_2002_06' created from 'helium/VRA_2002_06.csv'.\n",
      "Table 'VRA_2000_12' created from 'helium/VRA_2000_12.csv'.\n",
      "Table 'VRA_2015_02' created from 'helium/VRA_2015_02.csv'.\n",
      "Table 'VRA_2003_11' created from 'helium/VRA_2003_11.csv'.\n",
      "Table 'VRA_2021_12' created from 'helium/VRA_2021_12.csv'.\n",
      "Table 'VRA_2021_01' created from 'helium/VRA_2021_01.csv'.\n",
      "Table 'VRA_2005_09' created from 'helium/VRA_2005_09.csv'.\n",
      "Table 'VRA_2019_02' created from 'helium/VRA_2019_02.csv'.\n",
      "Table 'VRA_2004_01' created from 'helium/VRA_2004_01.csv'.\n",
      "Table 'VRA_2009_05' created from 'helium/VRA_2009_05.csv'.\n",
      "Table 'VRA_2021_03' created from 'helium/VRA_2021_03.csv'.\n",
      "Table 'VRA_2005_05' created from 'helium/VRA_2005_05.csv'.\n",
      "Table 'VRA_2002_05' created from 'helium/VRA_2002_05.csv'.\n",
      "Table 'VRA_2022_11' created from 'helium/VRA_2022_11.csv'.\n",
      "Table 'VRA_2008_08' created from 'helium/VRA_2008_08.csv'.\n",
      "Table 'VRA_2011_05' created from 'helium/VRA_2011_05.csv'.\n",
      "Table 'VRA_2021_07' created from 'helium/VRA_2021_07.csv'.\n",
      "Table 'VRA_2008_12' created from 'helium/VRA_2008_12.csv'.\n",
      "Table 'VRA_2001_02' created from 'helium/VRA_2001_02.csv'.\n",
      "Table 'VRA_2000_06' created from 'helium/VRA_2000_06.csv'.\n",
      "Table 'VRA_2003_06' created from 'helium/VRA_2003_06.csv'.\n",
      "Table 'VRA_2009_02' created from 'helium/VRA_2009_02.csv'.\n",
      "Table 'VRA_2000_03' created from 'helium/VRA_2000_03.csv'.\n",
      "Table 'VRA_2008_04' created from 'helium/VRA_2008_04.csv'.\n",
      "Table 'VRA_2020_06' created from 'helium/VRA_2020_06.csv'.\n",
      "Table 'VRA_2012_09' created from 'helium/VRA_2012_09.csv'.\n",
      "Table 'VRA_2006_01' created from 'helium/VRA_2006_01.csv'.\n",
      "Table 'VRA_2013_03' created from 'helium/VRA_2013_03.csv'.\n",
      "Table 'VRA_2007_01' created from 'helium/VRA_2007_01.csv'.\n",
      "Table 'VRA_2006_07' created from 'helium/VRA_2006_07.csv'.\n",
      "Table 'VRA_2024_02' created from 'helium/VRA_2024_02.csv'.\n",
      "Table 'VRA_2019_11' created from 'helium/VRA_2019_11.csv'.\n",
      "Table 'VRA_2014_08' created from 'helium/VRA_2014_08.csv'.\n",
      "Table 'VRA_2020_07' created from 'helium/VRA_2020_07.csv'.\n",
      "Table 'VRA_2022_01' created from 'helium/VRA_2022_01.csv'.\n",
      "Table 'VRA_2011_12' created from 'helium/VRA_2011_12.csv'.\n",
      "Table 'VRA_2014_06' created from 'helium/VRA_2014_06.csv'.\n",
      "Table 'VRA_2014_11' created from 'helium/VRA_2014_11.csv'.\n",
      "Table 'VRA_2001_04' created from 'helium/VRA_2001_04.csv'.\n",
      "Table 'VRA_2017_05' created from 'helium/VRA_2017_05.csv'.\n",
      "Table 'VRA_2012_04' created from 'helium/VRA_2012_04.csv'.\n",
      "Table 'VRA_2016_11' created from 'helium/VRA_2016_11.csv'.\n",
      "Table 'VRA_2007_06' created from 'helium/VRA_2007_06.csv'.\n",
      "Table 'VRA_2023_09' created from 'helium/VRA_2023_09.csv'.\n",
      "Table 'VRA_2022_03' created from 'helium/VRA_2022_03.csv'.\n",
      "Table 'VRA_2010_07' created from 'helium/VRA_2010_07.csv'.\n",
      "Table 'VRA_2001_07' created from 'helium/VRA_2001_07.csv'.\n",
      "Table 'VRA_2022_09' created from 'helium/VRA_2022_09.csv'.\n",
      "Table 'VRA_2017_02' created from 'helium/VRA_2017_02.csv'.\n",
      "Table 'VRA_2022_10' created from 'helium/VRA_2022_10.csv'.\n",
      "Table 'VRA_2010_05' created from 'helium/VRA_2010_05.csv'.\n",
      "Table 'VRA_2009_07' created from 'helium/VRA_2009_07.csv'.\n",
      "Table 'VRA_2008_11' created from 'helium/VRA_2008_11.csv'.\n",
      "Table 'VRA_2007_03' created from 'helium/VRA_2007_03.csv'.\n",
      "Table 'VRA_2015_09' created from 'helium/VRA_2015_09.csv'.\n",
      "Table 'VRA_2004_10' created from 'helium/VRA_2004_10.csv'.\n",
      "Table 'VRA_2010_08' created from 'helium/VRA_2010_08.csv'.\n",
      "Table 'VRA_2024_08' created from 'helium/VRA_2024_08.csv'.\n",
      "Table 'VRA_2002_10' created from 'helium/VRA_2002_10.csv'.\n",
      "Table 'VRA_2022_05' created from 'helium/VRA_2022_05.csv'.\n",
      "Table 'VRA_2011_06' created from 'helium/VRA_2011_06.csv'.\n",
      "Table 'VRA_2000_11' created from 'helium/VRA_2000_11.csv'.\n",
      "Table 'VRA_2016_07' created from 'helium/VRA_2016_07.csv'.\n",
      "Table 'VRA_2002_08' created from 'helium/VRA_2002_08.csv'.\n",
      "Table 'VRA_2015_06' created from 'helium/VRA_2015_06.csv'.\n",
      "Table 'VRA_2023_08' created from 'helium/VRA_2023_08.csv'.\n",
      "Table 'VRA_2009_12' created from 'helium/VRA_2009_12.csv'.\n",
      "Table 'VRA_2022_12' created from 'helium/VRA_2022_12.csv'.\n",
      "Table 'VRA_2003_03' created from 'helium/VRA_2003_03.csv'.\n",
      "Table 'VRA_2011_01' created from 'helium/VRA_2011_01.csv'.\n",
      "Table 'VRA_2019_04' created from 'helium/VRA_2019_04.csv'.\n",
      "Table 'VRA_2010_10' created from 'helium/VRA_2010_10.csv'.\n",
      "Table 'VRA_2021_06' created from 'helium/VRA_2021_06.csv'.\n",
      "Table 'VRA_2009_06' created from 'helium/VRA_2009_06.csv'.\n",
      "Table 'VRA_2000_09' created from 'helium/VRA_2000_09.csv'.\n",
      "Table 'VRA_2014_07' created from 'helium/VRA_2014_07.csv'.\n",
      "Table 'VRA_2011_10' created from 'helium/VRA_2011_10.csv'.\n",
      "Table 'VRA_2023_11' created from 'helium/VRA_2023_11.csv'.\n",
      "Table 'VRA_2001_10' created from 'helium/VRA_2001_10.csv'.\n",
      "Table 'VRA_2019_10' created from 'helium/VRA_2019_10.csv'.\n",
      "Table 'VRA_2007_12' created from 'helium/VRA_2007_12.csv'.\n",
      "Table 'VRA_2005_08' created from 'helium/VRA_2005_08.csv'.\n",
      "Table 'VRA_2020_09' created from 'helium/VRA_2020_09.csv'.\n",
      "Table 'VRA_2004_09' created from 'helium/VRA_2004_09.csv'.\n",
      "Table 'VRA_2008_05' created from 'helium/VRA_2008_05.csv'.\n",
      "Table 'VRA_2020_05' created from 'helium/VRA_2020_05.csv'.\n",
      "Table 'VRA_2014_12' created from 'helium/VRA_2014_12.csv'.\n",
      "Table 'VRA_2019_07' created from 'helium/VRA_2019_07.csv'.\n",
      "Table 'VRA_2009_10' created from 'helium/VRA_2009_10.csv'.\n",
      "Table 'VRA_2017_08' created from 'helium/VRA_2017_08.csv'.\n",
      "Table 'VRA_2000_02' created from 'helium/VRA_2000_02.csv'.\n",
      "Table 'VRA_2006_08' created from 'helium/VRA_2006_08.csv'.\n",
      "Table 'VRA_2015_10' created from 'helium/VRA_2015_10.csv'.\n",
      "Table 'VRA_2019_06' created from 'helium/VRA_2019_06.csv'.\n",
      "Table 'VRA_2003_10' created from 'helium/VRA_2003_10.csv'.\n",
      "Table 'VRA_2005_01' created from 'helium/VRA_2005_01.csv'.\n",
      "Table 'VRA_2021_02' created from 'helium/VRA_2021_02.csv'.\n",
      "Table 'VRA_2003_08' created from 'helium/VRA_2003_08.csv'.\n",
      "Table 'VRA_2013_05' created from 'helium/VRA_2013_05.csv'.\n",
      "Table 'VRA_2001_03' created from 'helium/VRA_2001_03.csv'.\n",
      "Table 'VRA_2023_05' created from 'helium/VRA_2023_05.csv'.\n",
      "Table 'VRA_2021_11' created from 'helium/VRA_2021_11.csv'.\n",
      "Table 'VRA_2008_02' created from 'helium/VRA_2008_02.csv'.\n",
      "Table 'VRA_2000_08' created from 'helium/VRA_2000_08.csv'.\n",
      "Table 'VRA_2003_05' created from 'helium/VRA_2003_05.csv'.\n",
      "Table 'VRA_2018_08' created from 'helium/VRA_2018_08.csv'.\n",
      "Table 'VRA_2018_01' created from 'helium/VRA_2018_01.csv'.\n",
      "Table 'VRA_2023_12' created from 'helium/VRA_2023_12.csv'.\n",
      "Table 'VRA_2021_08' created from 'helium/VRA_2021_08.csv'.\n",
      "Table 'VRA_2009_04' created from 'helium/VRA_2009_04.csv'.\n",
      "Table 'VRA_2015_08' created from 'helium/VRA_2015_08.csv'.\n",
      "Table 'VRA_2000_07' created from 'helium/VRA_2000_07.csv'.\n",
      "Table 'VRA_2007_05' created from 'helium/VRA_2007_05.csv'.\n",
      "Table 'VRA_2012_06' created from 'helium/VRA_2012_06.csv'.\n",
      "Table 'VRA_2011_07' created from 'helium/VRA_2011_07.csv'.\n",
      "Table 'VRA_2014_02' created from 'helium/VRA_2014_02.csv'.\n",
      "Table 'VRA_2002_12' created from 'helium/VRA_2002_12.csv'.\n",
      "Table 'VRA_2021_09' created from 'helium/VRA_2021_09.csv'.\n",
      "Table 'VRA_2021_05' created from 'helium/VRA_2021_05.csv'.\n",
      "Table 'VRA_2006_12' created from 'helium/VRA_2006_12.csv'.\n",
      "Table 'VRA_2015_05' created from 'helium/VRA_2015_05.csv'.\n",
      "Table 'VRA_2016_03' created from 'helium/VRA_2016_03.csv'.\n",
      "Table 'VRA_2018_07' created from 'helium/VRA_2018_07.csv'.\n",
      "Table 'VRA_2013_12' created from 'helium/VRA_2013_12.csv'.\n",
      "Table 'VRA_2007_09' created from 'helium/VRA_2007_09.csv'.\n",
      "Table 'VRA_2009_03' created from 'helium/VRA_2009_03.csv'.\n",
      "Table 'VRA_2016_09' created from 'helium/VRA_2016_09.csv'.\n",
      "Table 'VRA_2019_08' created from 'helium/VRA_2019_08.csv'.\n",
      "Table 'VRA_2005_11' created from 'helium/VRA_2005_11.csv'.\n",
      "Table 'VRA_2013_09' created from 'helium/VRA_2013_09.csv'.\n",
      "Table 'VRA_2014_01' created from 'helium/VRA_2014_01.csv'.\n",
      "Table 'VRA_2006_09' created from 'helium/VRA_2006_09.csv'.\n",
      "Table 'VRA_2010_12' created from 'helium/VRA_2010_12.csv'.\n",
      "Table 'VRA_2008_01' created from 'helium/VRA_2008_01.csv'.\n",
      "Table 'VRA_2000_05' created from 'helium/VRA_2000_05.csv'.\n",
      "Table 'VRA_2001_11' created from 'helium/VRA_2001_11.csv'.\n",
      "Table 'VRA_2001_09' created from 'helium/VRA_2001_09.csv'.\n",
      "Table 'VRA_2024_09' created from 'helium/VRA_2024_09.csv'.\n",
      "Table 'VRA_2004_02' created from 'helium/VRA_2004_02.csv'.\n",
      "Table 'VRA_2015_12' created from 'helium/VRA_2015_12.csv'.\n",
      "Table 'VRA_2024_10' created from 'helium/VRA_2024_10.csv'.\n",
      "Table 'VRA_2023_07' created from 'helium/VRA_2023_07.csv'.\n",
      "Table 'VRA_2013_02' created from 'helium/VRA_2013_02.csv'.\n",
      "Table 'VRA_2018_10' created from 'helium/VRA_2018_10.csv'.\n",
      "Table 'VRA_2023_01' created from 'helium/VRA_2023_01.csv'.\n",
      "Table 'VRA_2011_09' created from 'helium/VRA_2011_09.csv'.\n",
      "Table 'VRA_2007_04' created from 'helium/VRA_2007_04.csv'.\n",
      "Table 'VRA_2001_05' created from 'helium/VRA_2001_05.csv'.\n",
      "Table 'VRA_2025_01' created from 'helium/VRA_2025_01.csv'.\n",
      "Table 'VRA_2020_04' created from 'helium/VRA_2020_04.csv'.\n",
      "Table 'VRA_2013_01' created from 'helium/VRA_2013_01.csv'.\n",
      "Table 'VRA_2003_02' created from 'helium/VRA_2003_02.csv'.\n",
      "Table 'VRA_2024_07' created from 'helium/VRA_2024_07.csv'.\n",
      "Table 'VRA_2015_01' created from 'helium/VRA_2015_01.csv'.\n",
      "Table 'VRA_2013_08' created from 'helium/VRA_2013_08.csv'.\n",
      "Table 'VRA_2024_06' created from 'helium/VRA_2024_06.csv'.\n",
      "Table 'VRA_2017_06' created from 'helium/VRA_2017_06.csv'.\n",
      "Table 'VRA_2016_02' created from 'helium/VRA_2016_02.csv'.\n",
      "Table 'VRA_2019_09' created from 'helium/VRA_2019_09.csv'.\n",
      "Table 'VRA_2005_10' created from 'helium/VRA_2005_10.csv'.\n",
      "Table 'VRA_2015_03' created from 'helium/VRA_2015_03.csv'.\n",
      "Table 'VRA_2018_11' created from 'helium/VRA_2018_11.csv'.\n",
      "Table 'VRA_2002_07' created from 'helium/VRA_2002_07.csv'.\n",
      "Table 'VRA_2023_06' created from 'helium/VRA_2023_06.csv'.\n",
      "Table 'VRA_2018_06' created from 'helium/VRA_2018_06.csv'.\n",
      "Table 'VRA_2006_04' created from 'helium/VRA_2006_04.csv'.\n",
      "Table 'VRA_2012_02' created from 'helium/VRA_2012_02.csv'.\n",
      "Table 'VRA_2014_04' created from 'helium/VRA_2014_04.csv'.\n",
      "Table 'VRA_2005_06' created from 'helium/VRA_2005_06.csv'.\n",
      "Table 'VRA_2001_08' created from 'helium/VRA_2001_08.csv'.\n",
      "Table 'VRA_2008_10' created from 'helium/VRA_2008_10.csv'.\n",
      "Table 'VRA_2016_04' created from 'helium/VRA_2016_04.csv'.\n",
      "Table 'VRA_2010_09' created from 'helium/VRA_2010_09.csv'.\n",
      "Table 'VRA_2018_09' created from 'helium/VRA_2018_09.csv'.\n",
      "Table 'VRA_2009_09' created from 'helium/VRA_2009_09.csv'.\n",
      "Table 'VRA_2008_07' created from 'helium/VRA_2008_07.csv'.\n",
      "Table 'VRA_2016_12' created from 'helium/VRA_2016_12.csv'.\n",
      "Table 'VRA_2015_04' created from 'helium/VRA_2015_04.csv'.\n",
      "Table 'VRA_2012_11' created from 'helium/VRA_2012_11.csv'.\n",
      "Table 'VRA_2019_12' created from 'helium/VRA_2019_12.csv'.\n",
      "Table 'VRA_2000_01' created from 'helium/VRA_2000_01.csv'.\n",
      "Table 'VRA_2009_11' created from 'helium/VRA_2009_11.csv'.\n",
      "Table 'VRA_2014_10' created from 'helium/VRA_2014_10.csv'.\n",
      "Table 'VRA_2008_09' created from 'helium/VRA_2008_09.csv'.\n",
      "Table 'VRA_2012_10' created from 'helium/VRA_2012_10.csv'.\n",
      "Table 'VRA_2022_02' created from 'helium/VRA_2022_02.csv'.\n",
      "Table 'VRA_2006_06' created from 'helium/VRA_2006_06.csv'.\n",
      "Table 'VRA_2012_12' created from 'helium/VRA_2012_12.csv'.\n",
      "Table 'VRA_2007_10' created from 'helium/VRA_2007_10.csv'.\n",
      "Table 'VRA_2004_07' created from 'helium/VRA_2004_07.csv'.\n",
      "Table 'VRA_2005_07' created from 'helium/VRA_2005_07.csv'.\n",
      "Table 'VRA_2004_12' created from 'helium/VRA_2004_12.csv'.\n",
      "Table 'VRA_2007_08' created from 'helium/VRA_2007_08.csv'.\n",
      "Table 'VRA_2022_06' created from 'helium/VRA_2022_06.csv'.\n",
      "Table 'VRA_2010_01' created from 'helium/VRA_2010_01.csv'.\n",
      "Table 'VRA_2002_04' created from 'helium/VRA_2002_04.csv'.\n",
      "Table 'VRA_2004_06' created from 'helium/VRA_2004_06.csv'.\n",
      "Table 'VRA_2004_03' created from 'helium/VRA_2004_03.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Connect to (or create) the DuckDB database file named flight_data.duckdb\n",
    "con = duckdb.connect(\"flight_data.duckdb\")\n",
    "\n",
    "# Define the folder containing CSV files\n",
    "csv_folder = \"helium\"\n",
    "\n",
    "# Find all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "\n",
    "# Loop over each CSV file and create a table from it\n",
    "for csv_file in csv_files:\n",
    "    # Use the file name (without extension) as the table name\n",
    "    table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    \n",
    "    # Create the table by reading the CSV file and converting all values to strings\n",
    "    query = f\"\"\"\n",
    "    CREATE TABLE {table_name} AS\n",
    "    SELECT * FROM read_csv_auto('{csv_file}', all_varchar=True);\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    print(f\"Table '{table_name}' created from '{csv_file}'.\")\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc8d1b19-f271-471c-9ff5-1c476b9ece41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec421b924404ef480511b380130a5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in 'flights' table: 24315692\n"
     ]
    }
   ],
   "source": [
    "# Connect to the existing DuckDB database\n",
    "con = duckdb.connect(\"flight_data.duckdb\")\n",
    "\n",
    "# Get a list of all user-created tables in the main schema\n",
    "# (Excluding system tables and ignoring an existing 'flights' table, if any)\n",
    "tables = con.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'main'\n",
    "      AND table_name NOT IN ('flights')\n",
    "\"\"\").fetchall()\n",
    "\n",
    "# Build a UNION ALL query to merge all tables into one\n",
    "# We'll create (or replace) a table named 'flights'\n",
    "union_parts = []\n",
    "for (table_name,) in tables:\n",
    "    union_parts.append(f\"SELECT * FROM {table_name}\")\n",
    "\n",
    "# If no tables were found, you might want to handle that separately\n",
    "if not union_parts:\n",
    "    print(\"No user tables found to merge.\")\n",
    "else:\n",
    "    union_query = \"\"\"\n",
    "        CREATE OR REPLACE TABLE flights AS\n",
    "        {}\n",
    "    \"\"\".format(\"\\nUNION ALL\\n\".join(union_parts))\n",
    "\n",
    "    # Execute the query to merge all tables into 'flights'\n",
    "    con.execute(union_query)\n",
    "\n",
    "    # Count how many rows ended up in the new 'flights' table\n",
    "    row_count = con.execute(\"SELECT COUNT(*) FROM flights;\").fetchone()[0]\n",
    "    print(f\"Number of rows in 'flights' table: {row_count}\")\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82377c90-f73e-4f97-8886-0fa8bb69be43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3868adcb244b52bcf4aa512f9e0068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 24315692\n",
      "Distinct rows: 24288657\n",
      "Duplicates: 27035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fdb192ffa842678798eded788af6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New total rows after removing duplicates: 24288657\n"
     ]
    }
   ],
   "source": [
    "# Connect to your DuckDB database\n",
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# 1. Count total rows\n",
    "total_rows = con.execute(\"SELECT COUNT(*) FROM flights;\").fetchone()[0]\n",
    "\n",
    "# 2. Count distinct rows\n",
    "distinct_rows = con.execute(\"\"\"\n",
    "    SELECT COUNT(*) \n",
    "    FROM (SELECT DISTINCT * FROM flights) AS temp\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "duplicates = total_rows - distinct_rows\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Distinct rows: {distinct_rows}\")\n",
    "print(f\"Duplicates: {duplicates}\")\n",
    "\n",
    "# 3. Remove duplicates by recreating 'flights' with DISTINCT rows only\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE flights AS\n",
    "SELECT DISTINCT * \n",
    "FROM flights;\n",
    "\"\"\")\n",
    "\n",
    "# Verify the new row count after deduplication\n",
    "new_total_rows = con.execute(\"SELECT COUNT(*) FROM flights;\").fetchone()[0]\n",
    "print(f\"New total rows after removing duplicates: {new_total_rows}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e593302d-f251-4c2e-aa35-1e915e05bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column 'empresa_aerea'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87dbc03f6af4a1a859981908c8c3c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column 'long_empresa_aerea'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca0fd5c003348abb9aaf577247f8426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column 'numero_voo'.\n",
      "Cleaned column 'codigo_DI'.\n",
      "Cleaned column 'codigo_tipo_linha'.\n",
      "Cleaned column 'modelo_equipamento'.\n",
      "Cleaned column 'numero_assentos'.\n",
      "Cleaned column 'aeroporto_origem'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27055054ce064aedb6993d01eebeab0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column 'descricao_aeroporto_origem'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e92480e3d9f48379946fa329271cb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column 'partida_prevista'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788053dce48c4032ab68146866378730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column 'partida_real'.\n",
      "Cleaned column 'aeroporto_destino'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38eaaf2861f4747b0293a3b83a24b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column 'descricao_aeroporto_destino'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a67046788b84a219071db3f048bc0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column 'chegada_prevista'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46204b9070a4cc48b435236f3759655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column 'chegada_real'.\n",
      "Cleaned column 'situacao_voo'.\n",
      "Cleaned column 'justificativa'.\n",
      "Cleaned column 'referencia'.\n",
      "Cleaned column 'situacao_partida'.\n",
      "Cleaned column 'situacao_chegada'.\n",
      "Cleaned column 'ano'.\n",
      "Cleaned column 'mes'.\n",
      "Sample rows after cleaning:\n",
      "('VRG', None, '2206', '0', 'N', None, None, 'SBGL', None, '19/02/2001 19:00', None, 'SBBR', None, '19/02/2001 20:31', None, 'REALIZADO', 'N/A', None, None, None, '2001', '02')\n",
      "('TAM', None, '3941', '0', 'N', None, None, 'SBGL', None, '20/02/2001 16:45', None, 'SBGR', None, '20/02/2001 17:45', None, 'REALIZADO', 'N/A', None, None, None, '2001', '02')\n",
      "('VRG', None, '2261', '0', 'N', None, None, 'SBSL', None, '20/02/2001 11:10', None, 'SBFZ', None, '20/02/2001 12:25', None, 'REALIZADO', 'N/A', None, None, None, '2001', '02')\n",
      "('RSL', None, '5373', '0', 'E', None, None, 'SBBH', None, '20/02/2001 09:34', None, 'SBRJ', None, '20/02/2001 10:21', None, 'REALIZADO', 'N/A', None, None, None, '2001', '02')\n",
      "('RSL', None, '5000', '0', 'E', None, None, 'SBRJ', None, '20/02/2001 07:06', None, 'SBSP', None, '20/02/2001 07:55', None, 'REALIZADO', 'N/A', None, None, None, '2001', '02')\n",
      "('RSL', None, '5153', '0', 'R', None, None, 'SBGO', None, '20/02/2001 18:04', None, 'SBUL', None, '20/02/2001 18:33', None, 'REALIZADO', 'N/A', None, None, None, '2001', '02')\n",
      "('RSL', None, '5435', '0', 'R', None, None, 'SBSJ', None, '20/02/2001 14:42', None, 'SBRJ', None, '20/02/2001 15:43', None, 'REALIZADO', 'N/A', None, None, None, '2001', '02')\n",
      "('TAM', None, '3202', '0', 'E', None, None, 'SBRJ', None, '20/02/2001 08:31', None, 'SBBH', None, '20/02/2001 09:17', None, 'REALIZADO', 'N/A', None, None, None, '2001', '02')\n",
      "('RLE', None, '4808', '0', 'R', None, None, 'SWMW', None, '20/02/2001 09:00', None, 'SWPI', None, '20/02/2001 09:20', None, 'REALIZADO', 'N/A', None, None, None, '2001', '02')\n",
      "('VRG', None, '9950', 'B', 'N', None, None, 'SBGL', None, None, '21/02/2001 17:10', 'SBGL', None, None, '21/02/2001 18:35', 'REALIZADO', 'VE', None, None, None, '2001', '02')\n"
     ]
    }
   ],
   "source": [
    "# Connect to the DuckDB database\n",
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# Get column info from the 'flights' table\n",
    "schema_info = con.execute(\"PRAGMA table_info('flights');\").fetchall()\n",
    "\n",
    "# Loop over each column and update only string-type columns\n",
    "for col_index, col_name, col_type, *rest in schema_info:\n",
    "    if col_type.upper().startswith(\"VARCHAR\") or col_type.upper().startswith(\"STRING\"):\n",
    "        # Replace sequences of quotes with an empty string, then use NULLIF to set it to NULL if empty.\n",
    "        # Note the regex pattern '[''\"]+' uses doubled single quotes to represent a literal single quote.\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE flights\n",
    "            SET \"{col_name}\" = NULLIF(regexp_replace(\"{col_name}\", '[''\"]+', ''), '')\n",
    "        \"\"\"\n",
    "        con.execute(update_query)\n",
    "        print(f\"Cleaned column '{col_name}'.\")\n",
    "\n",
    "# Optionally, check the results (for example, view the first few rows)\n",
    "result = con.execute(\"SELECT * FROM flights LIMIT 10;\").fetchall()\n",
    "print(\"Sample rows after cleaning:\")\n",
    "for row in result:\n",
    "    print(row)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74fa8c35-8a2c-4078-a3ea-1c31904a2f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with at least one empty string: 0\n"
     ]
    }
   ],
   "source": [
    "# Connect to your DuckDB database\n",
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# Retrieve schema information from the 'flights' table\n",
    "schema_info = con.execute(\"PRAGMA table_info('flights');\").fetchall()\n",
    "\n",
    "# Extract the column names from the schema info\n",
    "# Assuming all columns are string columns (loaded as VARCHAR/STRING)\n",
    "columns = [col[1] for col in schema_info]\n",
    "\n",
    "# Construct a WHERE condition that checks if any column is an empty string\n",
    "# This will generate a condition like: \"col1\" = '' OR \"col2\" = '' OR ...\n",
    "condition = \" OR \".join([f'\"{col}\" = \\'\\' ' for col in columns])\n",
    "\n",
    "# Build the full SQL query to count the rows that have at least one empty string\n",
    "query = f\"SELECT COUNT(*) FROM flights WHERE {condition};\"\n",
    "\n",
    "# Execute the query and fetch the result\n",
    "empty_rows_count = con.execute(query).fetchone()[0]\n",
    "print(f\"Number of rows with at least one empty string: {empty_rows_count}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a147ed8-c01b-4f3e-a11a-70581ddc31a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the 'flights' table:\n",
      "    cid                         name     type  notnull dflt_value     pk\n",
      "0     0                empresa_aerea  VARCHAR    False       None  False\n",
      "1     1           long_empresa_aerea  VARCHAR    False       None  False\n",
      "2     2                   numero_voo  VARCHAR    False       None  False\n",
      "3     3                    codigo_DI  VARCHAR    False       None  False\n",
      "4     4            codigo_tipo_linha  VARCHAR    False       None  False\n",
      "5     5           modelo_equipamento  VARCHAR    False       None  False\n",
      "6     6              numero_assentos  VARCHAR    False       None  False\n",
      "7     7             aeroporto_origem  VARCHAR    False       None  False\n",
      "8     8   descricao_aeroporto_origem  VARCHAR    False       None  False\n",
      "9     9             partida_prevista  VARCHAR    False       None  False\n",
      "10   10                 partida_real  VARCHAR    False       None  False\n",
      "11   11            aeroporto_destino  VARCHAR    False       None  False\n",
      "12   12  descricao_aeroporto_destino  VARCHAR    False       None  False\n",
      "13   13             chegada_prevista  VARCHAR    False       None  False\n",
      "14   14                 chegada_real  VARCHAR    False       None  False\n",
      "15   15                 situacao_voo  VARCHAR    False       None  False\n",
      "16   16                justificativa  VARCHAR    False       None  False\n",
      "17   17                   referencia  VARCHAR    False       None  False\n",
      "18   18             situacao_partida  VARCHAR    False       None  False\n",
      "19   19             situacao_chegada  VARCHAR    False       None  False\n",
      "20   20                          ano  VARCHAR    False       None  False\n",
      "21   21                          mes  VARCHAR    False       None  False\n",
      "\n",
      "Number of rows: 24288657\n",
      "Number of columns: 22\n"
     ]
    }
   ],
   "source": [
    "# Connect to the DuckDB database\n",
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# Retrieve schema information using PRAGMA table_info, which includes:\n",
    "# cid, name, type, notnull, dflt_value, and pk for each column\n",
    "schema_df = con.execute(\"PRAGMA table_info('flights');\").fetchdf()\n",
    "\n",
    "print(\"Schema of the 'flights' table:\")\n",
    "print(schema_df)\n",
    "\n",
    "# Count the number of rows in the flights table\n",
    "row_count = con.execute(\"SELECT COUNT(*) FROM flights;\").fetchone()[0]\n",
    "\n",
    "# The number of columns is the number of rows in the schema_df DataFrame\n",
    "num_columns = schema_df.shape[0]\n",
    "\n",
    "print(f\"\\nNumber of rows: {row_count}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122d5105-058b-4b77-9eee-b6af0c2f22ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Schema of the 'flights' table:\n",
      "    cid                         name     type  notnull dflt_value     pk\n",
      "0     0                empresa_aerea  VARCHAR    False       None  False\n",
      "1     1           long_empresa_aerea  VARCHAR    False       None  False\n",
      "2     2                   numero_voo  VARCHAR    False       None  False\n",
      "3     3                    codigo_DI  VARCHAR    False       None  False\n",
      "4     4            codigo_tipo_linha  VARCHAR    False       None  False\n",
      "5     5           modelo_equipamento  VARCHAR    False       None  False\n",
      "6     6              numero_assentos  INTEGER    False       None  False\n",
      "7     7             aeroporto_origem  VARCHAR    False       None  False\n",
      "8     8   descricao_aeroporto_origem  VARCHAR    False       None  False\n",
      "9     9             partida_prevista  VARCHAR    False       None  False\n",
      "10   10                 partida_real  VARCHAR    False       None  False\n",
      "11   11            aeroporto_destino  VARCHAR    False       None  False\n",
      "12   12  descricao_aeroporto_destino  VARCHAR    False       None  False\n",
      "13   13             chegada_prevista  VARCHAR    False       None  False\n",
      "14   14                 chegada_real  VARCHAR    False       None  False\n",
      "15   15                 situacao_voo  VARCHAR    False       None  False\n",
      "16   16                justificativa  VARCHAR    False       None  False\n",
      "17   17                   referencia  VARCHAR    False       None  False\n",
      "18   18             situacao_partida  VARCHAR    False       None  False\n",
      "19   19             situacao_chegada  VARCHAR    False       None  False\n",
      "20   20                          ano  INTEGER    False       None  False\n",
      "21   21                          mes  INTEGER    False       None  False\n"
     ]
    }
   ],
   "source": [
    "# Connect to your DuckDB database\n",
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# Alter each column from VARCHAR to INT\n",
    "# The USING clause ensures DuckDB knows how to transform the existing data.\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE flights \n",
    "    ALTER COLUMN numero_assentos \n",
    "    TYPE INT \n",
    "    USING CAST(numero_assentos AS INT)\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE flights \n",
    "    ALTER COLUMN ano \n",
    "    TYPE INT \n",
    "    USING CAST(ano AS INT)\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE flights \n",
    "    ALTER COLUMN mes \n",
    "    TYPE INT \n",
    "    USING CAST(mes AS INT)\n",
    "\"\"\")\n",
    "\n",
    "# (Optional) Show the updated schema\n",
    "updated_schema = con.execute(\"PRAGMA table_info('flights');\").fetchdf()\n",
    "print(\"Updated Schema of the 'flights' table:\")\n",
    "print(updated_schema)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26aa4480-3bfd-438a-90a1-e58958f5fde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated schema of the 'flights' table after dropping 'referencia':\n",
      "    cid                         name     type  notnull dflt_value     pk\n",
      "0     0                empresa_aerea  VARCHAR    False       None  False\n",
      "1     1           long_empresa_aerea  VARCHAR    False       None  False\n",
      "2     2                   numero_voo  VARCHAR    False       None  False\n",
      "3     3                    codigo_DI  VARCHAR    False       None  False\n",
      "4     4            codigo_tipo_linha  VARCHAR    False       None  False\n",
      "5     5           modelo_equipamento  VARCHAR    False       None  False\n",
      "6     6              numero_assentos  INTEGER    False       None  False\n",
      "7     7             aeroporto_origem  VARCHAR    False       None  False\n",
      "8     8   descricao_aeroporto_origem  VARCHAR    False       None  False\n",
      "9     9             partida_prevista  VARCHAR    False       None  False\n",
      "10   10                 partida_real  VARCHAR    False       None  False\n",
      "11   11            aeroporto_destino  VARCHAR    False       None  False\n",
      "12   12  descricao_aeroporto_destino  VARCHAR    False       None  False\n",
      "13   13             chegada_prevista  VARCHAR    False       None  False\n",
      "14   14                 chegada_real  VARCHAR    False       None  False\n",
      "15   15                 situacao_voo  VARCHAR    False       None  False\n",
      "16   16                justificativa  VARCHAR    False       None  False\n",
      "17   17             situacao_partida  VARCHAR    False       None  False\n",
      "18   18             situacao_chegada  VARCHAR    False       None  False\n",
      "19   19                          ano  INTEGER    False       None  False\n",
      "20   20                          mes  INTEGER    False       None  False\n"
     ]
    }
   ],
   "source": [
    "# Connect to your DuckDB database\n",
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# Drop the 'referencia' column from the 'flights' table\n",
    "con.execute(\"ALTER TABLE flights DROP COLUMN referencia;\")\n",
    "\n",
    "# (Optional) Verify that the column was removed\n",
    "updated_schema = con.execute(\"PRAGMA table_info('flights');\").fetchdf()\n",
    "print(\"Updated schema of the 'flights' table after dropping 'referencia':\")\n",
    "print(updated_schema)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e7922fa-c03e-4107-9a66-4e4a5e201592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f6874974b247e6bd753f74145c934c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated schema of the 'flights' table:\n",
      "    cid                         name     type  notnull dflt_value     pk\n",
      "0     0                          ano  INTEGER    False       None  False\n",
      "1     1                          mes  INTEGER    False       None  False\n",
      "2     2                empresa_aerea  VARCHAR    False       None  False\n",
      "3     3           long_empresa_aerea  VARCHAR    False       None  False\n",
      "4     4                   numero_voo  VARCHAR    False       None  False\n",
      "5     5                    codigo_DI  VARCHAR    False       None  False\n",
      "6     6            codigo_tipo_linha  VARCHAR    False       None  False\n",
      "7     7           modelo_equipamento  VARCHAR    False       None  False\n",
      "8     8              numero_assentos  INTEGER    False       None  False\n",
      "9     9             aeroporto_origem  VARCHAR    False       None  False\n",
      "10   10   descricao_aeroporto_origem  VARCHAR    False       None  False\n",
      "11   11             partida_prevista  VARCHAR    False       None  False\n",
      "12   12                 partida_real  VARCHAR    False       None  False\n",
      "13   13            aeroporto_destino  VARCHAR    False       None  False\n",
      "14   14  descricao_aeroporto_destino  VARCHAR    False       None  False\n",
      "15   15             chegada_prevista  VARCHAR    False       None  False\n",
      "16   16                 chegada_real  VARCHAR    False       None  False\n",
      "17   17                 situacao_voo  VARCHAR    False       None  False\n",
      "18   18                justificativa  VARCHAR    False       None  False\n",
      "19   19             situacao_partida  VARCHAR    False       None  False\n",
      "20   20             situacao_chegada  VARCHAR    False       None  False\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# 1. Get the current column list\n",
    "schema_info = con.execute(\"PRAGMA table_info('flights');\").fetchall()\n",
    "# schema_info is a list of tuples: (cid, name, type, notnull, dflt_value, pk)\n",
    "all_columns = [row[1] for row in schema_info]  # Extract just the column names\n",
    "\n",
    "# 2. Build a new column order: 'ano' first, 'mes' second, then the rest\n",
    "#    (Remove them first so we don't duplicate)\n",
    "all_columns.remove(\"ano\")\n",
    "all_columns.remove(\"mes\")\n",
    "new_order = [\"ano\", \"mes\"] + all_columns\n",
    "\n",
    "# 3. Create a temporary table in the new order\n",
    "#    We'll quote column names in case they have special characters.\n",
    "select_clause = \", \".join([f'\"{col}\"' for col in new_order])\n",
    "create_temp_table = f\"\"\"\n",
    "    CREATE TABLE flights_temp AS\n",
    "    SELECT {select_clause}\n",
    "    FROM flights\n",
    "\"\"\"\n",
    "con.execute(create_temp_table)\n",
    "\n",
    "# 4. Drop the old flights table\n",
    "con.execute(\"DROP TABLE flights\")\n",
    "\n",
    "# 5. Rename the temporary table to flights\n",
    "con.execute(\"ALTER TABLE flights_temp RENAME TO flights\")\n",
    "\n",
    "# (Optional) Show the updated schema\n",
    "updated_schema = con.execute(\"PRAGMA table_info('flights');\").fetchdf()\n",
    "print(\"Updated schema of the 'flights' table:\")\n",
    "print(updated_schema)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67f4ded0-1402-47ab-aaf5-c15529552b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06e973bf7ed4987a757c761f4bd4149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'flights' has been reordered by ano and mes.\n"
     ]
    }
   ],
   "source": [
    "# Connect to your DuckDB database\n",
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# Create a new temporary table ordered by ano and mes\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE flights_temp AS\n",
    "    SELECT * FROM flights\n",
    "    ORDER BY ano, mes;\n",
    "\"\"\")\n",
    "\n",
    "# Drop the old flights table\n",
    "con.execute(\"DROP TABLE flights;\")\n",
    "\n",
    "# Rename the temporary table to flights\n",
    "con.execute(\"ALTER TABLE flights_temp RENAME TO flights;\")\n",
    "\n",
    "print(\"Table 'flights' has been reordered by ano and mes.\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26a33b19-1d2c-4f5c-919a-39083d0e52d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows:\n",
      "    ano  mes empresa_aerea long_empresa_aerea numero_voo codigo_DI  \\\n",
      "0  2000    1           TBA               None       1690         0   \n",
      "1  2000    1           BLC               None       8230         0   \n",
      "2  2000    1           RSL               None       4100         0   \n",
      "3  2000    1           TAB               None       1620         0   \n",
      "4  2000    1           RLE               None       1070         0   \n",
      "5  2000    1           PEP               None       2900         0   \n",
      "6  2000    1           VRG               None       3260         0   \n",
      "7  2000    1           TBA               None       2000         0   \n",
      "8  2000    1           TBA               None       3120         0   \n",
      "9  2000    1           BLC               None        171         9   \n",
      "\n",
      "  codigo_tipo_linha modelo_equipamento  numero_assentos aeroporto_origem  ...  \\\n",
      "0                 N               None             <NA>             SBSV  ...   \n",
      "1                 N               None             <NA>             SBCT  ...   \n",
      "2                 R               None             <NA>             SBJV  ...   \n",
      "3                 R               None             <NA>             SWJN  ...   \n",
      "4                 R               None             <NA>             SWBR  ...   \n",
      "5                 R               None             <NA>             SBHT  ...   \n",
      "6                 N               None             <NA>             SBRF  ...   \n",
      "7                 N               None             <NA>             SBGR  ...   \n",
      "8                 N               None             <NA>             SBRJ  ...   \n",
      "9                 R               None             <NA>             SBMO  ...   \n",
      "\n",
      "   partida_prevista      partida_real aeroporto_destino  \\\n",
      "0  01/01/2000 22:20              None              SBGL   \n",
      "1  01/01/2000 18:20              None              SBFL   \n",
      "2  01/01/2000 07:52              None              SBFL   \n",
      "3  01/01/2000 09:00              None              SWJU   \n",
      "4  01/01/2000 11:10              None              SBEG   \n",
      "5  01/01/2000 10:30              None              SBMA   \n",
      "6  01/01/2000 20:15              None              SBFZ   \n",
      "7  01/01/2000 12:10              None              SBPA   \n",
      "8  01/01/2000 10:44              None              SBSP   \n",
      "9              None  01/01/2000 19:32              SBCT   \n",
      "\n",
      "  descricao_aeroporto_destino  chegada_prevista      chegada_real  \\\n",
      "0                        None  02/01/2000 00:11              None   \n",
      "1                        None  01/01/2000 19:00              None   \n",
      "2                        None  01/01/2000 08:27              None   \n",
      "3                        None  01/01/2000 09:30              None   \n",
      "4                        None  01/01/2000 11:40              None   \n",
      "5                        None  01/01/2000 11:30              None   \n",
      "6                        None  01/01/2000 21:30              None   \n",
      "7                        None  01/01/2000 13:41              None   \n",
      "8                        None  01/01/2000 11:35              None   \n",
      "9                        None              None  01/01/2000 22:54   \n",
      "\n",
      "  situacao_voo justificativa situacao_partida situacao_chegada  \n",
      "0    REALIZADO           N/A             None             None  \n",
      "1    REALIZADO           N/A             None             None  \n",
      "2    CANCELADO            XB             None             None  \n",
      "3    REALIZADO           N/A             None             None  \n",
      "4    CANCELADO            XA             None             None  \n",
      "5    CANCELADO            XL             None             None  \n",
      "6    REALIZADO           N/A             None             None  \n",
      "7    REALIZADO           N/A             None             None  \n",
      "8    REALIZADO           N/A             None             None  \n",
      "9    REALIZADO           N/A             None             None  \n",
      "\n",
      "[10 rows x 21 columns]\n",
      "\n",
      "Random 10 rows:\n",
      "    ano  mes empresa_aerea                                 long_empresa_aerea  \\\n",
      "0  2015   10           GLO  GOL LINHAS AREAS S.A. (EX- VRG LINHAS AREAS ...   \n",
      "1  2011    7           TAM                             TAM LINHAS AREAS S.A.   \n",
      "2  2016   11           GLO  GOL LINHAS AREAS S.A. (EX- VRG LINHAS AREAS ...   \n",
      "3  2016    1           GLO  GOL LINHAS AREAS S.A. (EX- VRG LINHAS AREAS ...   \n",
      "4  2011    5           GLO  GOL LINHAS AREAS S.A. (EX- VRG LINHAS AREAS ...   \n",
      "5  2024   11           SID                        SIDERAL LINHAS AREAS LTDA.   \n",
      "6  2006    8           GLO                                               None   \n",
      "7  2007   10           CMP                                               None   \n",
      "8  2023   12           TAM                             TAM LINHAS AREAS S.A.   \n",
      "9  2016    8           AZU                 AZUL LINHAS AREAS BRASILEIRAS S/A   \n",
      "\n",
      "  numero_voo codigo_DI codigo_tipo_linha modelo_equipamento  numero_assentos  \\\n",
      "0       1362         0                 N               B738              183   \n",
      "1       3254         0                 N               A320              174   \n",
      "2       1716         0                 N               B738              177   \n",
      "3       1340         0                 N               B738              177   \n",
      "4       1709         0                 N               B737              144   \n",
      "5       9775         0                 C               B73Y                0   \n",
      "6       1711         0                 N               None             <NA>   \n",
      "7       0759         0                 I               None             <NA>   \n",
      "8       3851         0                 N               A319              144   \n",
      "9       2482         0                 N               E195              118   \n",
      "\n",
      "  aeroporto_origem  ...  partida_prevista      partida_real aeroporto_destino  \\\n",
      "0             SBBV  ...  28/10/2015 03:40              None              SBEG   \n",
      "1             SBGL  ...  22/07/2011 23:32  22/07/2011 23:32              SBFZ   \n",
      "2             SBBR  ...  23/11/2016 21:10              None              SBCY   \n",
      "3             SBMA  ...  18/01/2016 14:53              None              SBBE   \n",
      "4             SBFN  ...  16/05/2011 15:35  16/05/2011 15:35              SBRF   \n",
      "5             SBGR  ...  22/11/2024 04:00  22/11/2024 03:55              SBRF   \n",
      "6             SBCF  ...  28/08/2006 21:10              None              SBSP   \n",
      "7             MPTO  ...  23/10/2007 22:54              None              SBGR   \n",
      "8             SBTE  ...  29/12/2023 04:15  29/12/2023 04:06              SBBR   \n",
      "9             SBCT  ...  09/08/2016 09:28  09/08/2016 09:20              SBPA   \n",
      "\n",
      "                         descricao_aeroporto_destino  chegada_prevista  \\\n",
      "0               EDUARDO GOMES - MANAUS - AM - BRASIL  28/10/2015 05:00   \n",
      "1            PINTO MARTINS - FORTALEZA - CE - BRASIL  23/07/2011 02:38   \n",
      "2      MARECHAL RONDON - VRZEA GRANDE - MT - BRASIL  23/11/2016 22:55   \n",
      "3  INTERNACIONAL DE BELM/VAL DE CANS/JLIO CEZAR...  18/01/2016 15:52   \n",
      "4  GUARARAPES - GILBERTO FREYRE - RECIFE - PE - B...  16/05/2011 16:41   \n",
      "5  GUARARAPES - GILBERTO FREYRE - RECIFE - PE - B...  22/11/2024 07:00   \n",
      "6                                               None  28/08/2006 22:25   \n",
      "7                                               None  24/10/2007 05:55   \n",
      "8  PRESIDENTE JUSCELINO KUBITSCHEK - BRASLIA - D...  29/12/2023 06:30   \n",
      "9         SALGADO FILHO - PORTO ALEGRE - RS - BRASIL  09/08/2016 10:40   \n",
      "\n",
      "       chegada_real situacao_voo                      justificativa  \\\n",
      "0              None    REALIZADO                               None   \n",
      "1  23/07/2011 02:38    REALIZADO                               None   \n",
      "2              None    REALIZADO                               None   \n",
      "3              None    REALIZADO                               None   \n",
      "4  16/05/2011 16:41    REALIZADO                               None   \n",
      "5  22/11/2024 07:00    REALIZADO                               None   \n",
      "6              None    REALIZADO                                N/A   \n",
      "7              None    CANCELADO                                 XN   \n",
      "8  29/12/2023 06:10    REALIZADO                               None   \n",
      "9  09/08/2016 10:35    REALIZADO  ANTECIPAO DE HORRIO AUTORIZADA   \n",
      "\n",
      "  situacao_partida situacao_chegada  \n",
      "0             None             None  \n",
      "1          Pontual          Pontual  \n",
      "2             None             None  \n",
      "3             None             None  \n",
      "4          Pontual          Pontual  \n",
      "5       Antecipado          Pontual  \n",
      "6             None             None  \n",
      "7             None             None  \n",
      "8       Antecipado       Antecipado  \n",
      "9       Antecipado       Antecipado  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Connect to your DuckDB database\n",
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# 1. Get the first 10 rows as a DataFrame\n",
    "df_first_10 = con.execute(\"SELECT * FROM flights LIMIT 10;\").fetchdf()\n",
    "print(\"First 10 rows:\")\n",
    "print(df_first_10)\n",
    "\n",
    "# 2. Get 10 random rows as a DataFrame\n",
    "df_random_10 = con.execute(\"SELECT * FROM flights ORDER BY RANDOM() LIMIT 10;\").fetchdf()\n",
    "print(\"\\nRandom 10 rows:\")\n",
    "print(df_random_10)\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ecc4e0f-aa88-47ed-8a6a-3c8595a2d5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   partida_prevista dia_partida_prevista hora_partida_prevista  \\\n",
      "0  01/01/2000 22:20           2000-01-01              22:20:00   \n",
      "1  01/01/2000 18:20           2000-01-01              18:20:00   \n",
      "2  01/01/2000 07:52           2000-01-01              07:52:00   \n",
      "3  01/01/2000 09:00           2000-01-01              09:00:00   \n",
      "4  01/01/2000 11:10           2000-01-01              11:10:00   \n",
      "5  01/01/2000 10:30           2000-01-01              10:30:00   \n",
      "6  01/01/2000 20:15           2000-01-01              20:15:00   \n",
      "7  01/01/2000 12:10           2000-01-01              12:10:00   \n",
      "8  01/01/2000 10:44           2000-01-01              10:44:00   \n",
      "9              None                  NaT                  <NA>   \n",
      "\n",
      "       partida_real dia_partida_real hora_partida_real  chegada_prevista  \\\n",
      "0              None              NaT              <NA>  02/01/2000 00:11   \n",
      "1              None              NaT              <NA>  01/01/2000 19:00   \n",
      "2              None              NaT              <NA>  01/01/2000 08:27   \n",
      "3              None              NaT              <NA>  01/01/2000 09:30   \n",
      "4              None              NaT              <NA>  01/01/2000 11:40   \n",
      "5              None              NaT              <NA>  01/01/2000 11:30   \n",
      "6              None              NaT              <NA>  01/01/2000 21:30   \n",
      "7              None              NaT              <NA>  01/01/2000 13:41   \n",
      "8              None              NaT              <NA>  01/01/2000 11:35   \n",
      "9  01/01/2000 19:32       2000-01-01          19:32:00              None   \n",
      "\n",
      "  dia_chegada_prevista hora_chegada_prevista      chegada_real  \\\n",
      "0           2000-01-02              00:11:00              None   \n",
      "1           2000-01-01              19:00:00              None   \n",
      "2           2000-01-01              08:27:00              None   \n",
      "3           2000-01-01              09:30:00              None   \n",
      "4           2000-01-01              11:40:00              None   \n",
      "5           2000-01-01              11:30:00              None   \n",
      "6           2000-01-01              21:30:00              None   \n",
      "7           2000-01-01              13:41:00              None   \n",
      "8           2000-01-01              11:35:00              None   \n",
      "9                  NaT                  <NA>  01/01/2000 22:54   \n",
      "\n",
      "  dia_chegada_real hora_chegada_real  \n",
      "0              NaT              <NA>  \n",
      "1              NaT              <NA>  \n",
      "2              NaT              <NA>  \n",
      "3              NaT              <NA>  \n",
      "4              NaT              <NA>  \n",
      "5              NaT              <NA>  \n",
      "6              NaT              <NA>  \n",
      "7              NaT              <NA>  \n",
      "8              NaT              <NA>  \n",
      "9       2000-01-01          22:54:00  \n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# Add new columns one by one\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN dia_partida_prevista DATE;\")\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN hora_partida_prevista TIME;\")\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN dia_partida_real DATE;\")\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN hora_partida_real TIME;\")\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN dia_chegada_prevista DATE;\")\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN hora_chegada_prevista TIME;\")\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN dia_chegada_real DATE;\")\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN hora_chegada_real TIME;\")\n",
    "\n",
    "# Update for partida_prevista: extract date and time parts\n",
    "con.execute(\"\"\"\n",
    "    UPDATE flights\n",
    "    SET \n",
    "        dia_partida_prevista = CASE\n",
    "            WHEN length(partida_prevista) >= 10\n",
    "                THEN CAST(strptime(substr(partida_prevista, 1, 10), '%d/%m/%Y') AS DATE)\n",
    "            ELSE NULL\n",
    "        END,\n",
    "        hora_partida_prevista = CASE\n",
    "            WHEN length(partida_prevista) > 10\n",
    "                THEN CAST(strptime(substr(partida_prevista, 12, 5), '%H:%M') AS TIME)\n",
    "            ELSE NULL\n",
    "        END\n",
    "    WHERE partida_prevista IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Update for partida_real: extract date and time parts\n",
    "con.execute(\"\"\"\n",
    "    UPDATE flights\n",
    "    SET \n",
    "        dia_partida_real = CASE\n",
    "            WHEN length(partida_real) >= 10\n",
    "                THEN CAST(strptime(substr(partida_real, 1, 10), '%d/%m/%Y') AS DATE)\n",
    "            ELSE NULL\n",
    "        END,\n",
    "        hora_partida_real = CASE\n",
    "            WHEN length(partida_real) > 10\n",
    "                THEN CAST(strptime(substr(partida_real, 12, 5), '%H:%M') AS TIME)\n",
    "            ELSE NULL\n",
    "        END\n",
    "    WHERE partida_real IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Update for chegada_prevista: extract date and time parts\n",
    "con.execute(\"\"\"\n",
    "    UPDATE flights\n",
    "    SET \n",
    "        dia_chegada_prevista = CASE\n",
    "            WHEN length(chegada_prevista) >= 10\n",
    "                THEN CAST(strptime(substr(chegada_prevista, 1, 10), '%d/%m/%Y') AS DATE)\n",
    "            ELSE NULL\n",
    "        END,\n",
    "        hora_chegada_prevista = CASE\n",
    "            WHEN length(chegada_prevista) > 10\n",
    "                THEN CAST(strptime(substr(chegada_prevista, 12, 5), '%H:%M') AS TIME)\n",
    "            ELSE NULL\n",
    "        END\n",
    "    WHERE chegada_prevista IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Update for chegada_real: extract date and time parts\n",
    "con.execute(\"\"\"\n",
    "    UPDATE flights\n",
    "    SET \n",
    "        dia_chegada_real = CASE\n",
    "            WHEN length(chegada_real) >= 10\n",
    "                THEN CAST(strptime(substr(chegada_real, 1, 10), '%d/%m/%Y') AS DATE)\n",
    "            ELSE NULL\n",
    "        END,\n",
    "        hora_chegada_real = CASE\n",
    "            WHEN length(chegada_real) > 10\n",
    "                THEN CAST(strptime(substr(chegada_real, 12, 5), '%H:%M') AS TIME)\n",
    "            ELSE NULL\n",
    "        END\n",
    "    WHERE chegada_real IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Verify the results by selecting a few rows\n",
    "df_check = con.execute(\"\"\"\n",
    "    SELECT partida_prevista, dia_partida_prevista, hora_partida_prevista,\n",
    "           partida_real,    dia_partida_real,    hora_partida_real,\n",
    "           chegada_prevista, dia_chegada_prevista, hora_chegada_prevista,\n",
    "           chegada_real,    dia_chegada_real,    hora_chegada_real\n",
    "    FROM flights\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df_check)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95971c39-abfe-4f84-8019-734b6234bce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dia_partida_prevista hora_partida_prevista dia_partida_real  \\\n",
      "0           2000-01-01              22:20:00              NaT   \n",
      "1           2000-01-01              18:20:00              NaT   \n",
      "2           2000-01-01              07:52:00              NaT   \n",
      "3           2000-01-01              09:00:00              NaT   \n",
      "4           2000-01-01              11:10:00              NaT   \n",
      "5           2000-01-01              10:30:00              NaT   \n",
      "6           2000-01-01              20:15:00              NaT   \n",
      "7           2000-01-01              12:10:00              NaT   \n",
      "8           2000-01-01              10:44:00              NaT   \n",
      "9                  NaT                  <NA>       2000-01-01   \n",
      "\n",
      "  hora_partida_real status_partida dia_chegada_prevista hora_chegada_prevista  \\\n",
      "0              <NA>     indefinido           2000-01-02              00:11:00   \n",
      "1              <NA>     indefinido           2000-01-01              19:00:00   \n",
      "2              <NA>     indefinido           2000-01-01              08:27:00   \n",
      "3              <NA>     indefinido           2000-01-01              09:30:00   \n",
      "4              <NA>     indefinido           2000-01-01              11:40:00   \n",
      "5              <NA>     indefinido           2000-01-01              11:30:00   \n",
      "6              <NA>     indefinido           2000-01-01              21:30:00   \n",
      "7              <NA>     indefinido           2000-01-01              13:41:00   \n",
      "8              <NA>     indefinido           2000-01-01              11:35:00   \n",
      "9          19:32:00     indefinido                  NaT                  <NA>   \n",
      "\n",
      "  dia_chegada_real hora_chegada_real status_chegada  \n",
      "0              NaT              <NA>     indefinido  \n",
      "1              NaT              <NA>     indefinido  \n",
      "2              NaT              <NA>     indefinido  \n",
      "3              NaT              <NA>     indefinido  \n",
      "4              NaT              <NA>     indefinido  \n",
      "5              NaT              <NA>     indefinido  \n",
      "6              NaT              <NA>     indefinido  \n",
      "7              NaT              <NA>     indefinido  \n",
      "8              NaT              <NA>     indefinido  \n",
      "9       2000-01-01          22:54:00     indefinido  \n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# 1) Add two new columns for the status\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN status_partida VARCHAR;\")\n",
    "con.execute(\"ALTER TABLE flights ADD COLUMN status_chegada VARCHAR;\")\n",
    "\n",
    "# 2) Update status_partida\n",
    "#    - We first check if any required columns are NULL (unknown).\n",
    "#    - Otherwise we compare (date + time) for real vs. scheduled.\n",
    "con.execute(\"\"\"\n",
    "    UPDATE flights\n",
    "    SET status_partida = CASE\n",
    "        WHEN dia_partida_prevista IS NULL OR hora_partida_prevista IS NULL\n",
    "             OR dia_partida_real IS NULL OR hora_partida_real IS NULL\n",
    "             THEN 'indefinido'\n",
    "        WHEN (dia_partida_real + hora_partida_real) > (dia_partida_prevista + hora_partida_prevista)\n",
    "             THEN 'atrasado'\n",
    "        WHEN (dia_partida_real + hora_partida_real) < (dia_partida_prevista + hora_partida_prevista)\n",
    "             THEN 'adiantado'\n",
    "        ELSE 'ok'\n",
    "    END\n",
    "\"\"\")\n",
    "\n",
    "# 3) Update status_chegada\n",
    "con.execute(\"\"\"\n",
    "    UPDATE flights\n",
    "    SET status_chegada = CASE\n",
    "        WHEN dia_chegada_prevista IS NULL OR hora_chegada_prevista IS NULL\n",
    "             OR dia_chegada_real IS NULL OR hora_chegada_real IS NULL\n",
    "             THEN 'indefinido'\n",
    "        WHEN (dia_chegada_real + hora_chegada_real) > (dia_chegada_prevista + hora_chegada_prevista)\n",
    "             THEN 'atrasado'\n",
    "        WHEN (dia_chegada_real + hora_chegada_real) < (dia_chegada_prevista + hora_chegada_prevista)\n",
    "             THEN 'adiantado'\n",
    "        ELSE 'ok'\n",
    "    END\n",
    "\"\"\")\n",
    "\n",
    "# (Optional) Verify a few rows\n",
    "df_check = con.execute(\"\"\"\n",
    "    SELECT dia_partida_prevista, hora_partida_prevista,\n",
    "           dia_partida_real, hora_partida_real,\n",
    "           status_partida,\n",
    "           dia_chegada_prevista, hora_chegada_prevista,\n",
    "           dia_chegada_real, hora_chegada_real,\n",
    "           status_chegada\n",
    "    FROM flights\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df_check)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "726894a6-d38b-451f-9755-4ca28d5d061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dia_partida_prevista hora_partida_prevista dia_partida_real  \\\n",
      "0           2011-02-14              09:09:00       2011-02-14   \n",
      "1           2011-02-14              06:05:00       2011-02-14   \n",
      "2           2011-02-15              07:03:00              NaT   \n",
      "3           2011-02-15              15:40:00              NaT   \n",
      "4           2011-02-15              16:50:00       2011-02-15   \n",
      "5           2011-02-16              10:00:00       2011-02-16   \n",
      "6           2011-02-17              13:58:00       2011-02-17   \n",
      "7           2011-02-17              20:10:00       2011-02-17   \n",
      "8           2011-02-17              03:00:00       2011-02-17   \n",
      "9           2011-02-17              05:20:00       2011-02-17   \n",
      "\n",
      "  hora_partida_real status_partida dia_chegada_prevista hora_chegada_prevista  \\\n",
      "0          09:09:00             ok           2011-02-14              10:22:00   \n",
      "1          06:05:00             ok           2011-02-14              09:50:00   \n",
      "2              <NA>     indefinido           2011-02-15              08:00:00   \n",
      "3              <NA>     indefinido           2011-02-15              18:29:00   \n",
      "4          17:29:00       atrasado           2011-02-15              18:40:00   \n",
      "5          10:00:00             ok           2011-02-16              11:00:00   \n",
      "6          13:58:00             ok           2011-02-17              15:30:00   \n",
      "7          20:10:00             ok           2011-02-17              20:57:00   \n",
      "8          02:58:00      adiantado           2011-02-17              06:05:00   \n",
      "9          05:20:00             ok           2011-02-17              06:15:00   \n",
      "\n",
      "  dia_chegada_real hora_chegada_real status_chegada  \n",
      "0       2011-02-14          10:22:00             ok  \n",
      "1       2011-02-14          09:50:00             ok  \n",
      "2              NaT              <NA>     indefinido  \n",
      "3              NaT              <NA>     indefinido  \n",
      "4       2011-02-15          19:10:00       atrasado  \n",
      "5       2011-02-16          11:00:00             ok  \n",
      "6       2011-02-17          15:30:00             ok  \n",
      "7       2011-02-17          20:57:00             ok  \n",
      "8       2011-02-17          06:37:00       atrasado  \n",
      "9       2011-02-17          06:15:00             ok  \n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# (Optional) Verify a few rows\n",
    "df_check = con.execute(\"\"\"\n",
    "    SELECT dia_partida_prevista, hora_partida_prevista,\n",
    "           dia_partida_real, hora_partida_real,\n",
    "           status_partida,\n",
    "           dia_chegada_prevista, hora_chegada_prevista,\n",
    "           dia_chegada_real, hora_chegada_real,\n",
    "           status_chegada\n",
    "    FROM flights\n",
    "    LIMIT 10 offset 10000000\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df_check)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2f03da8-bacc-4d5d-9540-7d5c9cd2c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the 'flights' table:\n",
      "    cid                         name       type  notnull dflt_value     pk\n",
      "0     0                          ano    INTEGER    False       None  False\n",
      "1     1                          mes    INTEGER    False       None  False\n",
      "2     2                empresa_aerea    VARCHAR    False       None  False\n",
      "3     3           long_empresa_aerea    VARCHAR    False       None  False\n",
      "4     4                   numero_voo    VARCHAR    False       None  False\n",
      "5     5                    codigo_DI    VARCHAR    False       None  False\n",
      "6     6            codigo_tipo_linha    VARCHAR    False       None  False\n",
      "7     7           modelo_equipamento    VARCHAR    False       None  False\n",
      "8     8              numero_assentos    INTEGER    False       None  False\n",
      "9     9             aeroporto_origem    VARCHAR    False       None  False\n",
      "10   10   descricao_aeroporto_origem    VARCHAR    False       None  False\n",
      "11   11             partida_prevista    VARCHAR    False       None  False\n",
      "12   12                 partida_real    VARCHAR    False       None  False\n",
      "13   13            aeroporto_destino    VARCHAR    False       None  False\n",
      "14   14  descricao_aeroporto_destino    VARCHAR    False       None  False\n",
      "15   15             chegada_prevista    VARCHAR    False       None  False\n",
      "16   16                 chegada_real    VARCHAR    False       None  False\n",
      "17   17                 situacao_voo    VARCHAR    False       None  False\n",
      "18   18                justificativa    VARCHAR    False       None  False\n",
      "19   19             situacao_partida    VARCHAR    False       None  False\n",
      "20   20             situacao_chegada    VARCHAR    False       None  False\n",
      "21   21          partida_prevista_ts  TIMESTAMP    False       None  False\n",
      "22   22              partida_real_ts  TIMESTAMP    False       None  False\n",
      "23   23          chegada_prevista_ts  TIMESTAMP    False       None  False\n",
      "24   24              chegada_real_ts  TIMESTAMP    False       None  False\n",
      "25   25         dia_partida_prevista       DATE    False       None  False\n",
      "26   26        hora_partida_prevista       TIME    False       None  False\n",
      "27   27             dia_partida_real       DATE    False       None  False\n",
      "28   28            hora_partida_real       TIME    False       None  False\n",
      "29   29         dia_chegada_prevista       DATE    False       None  False\n",
      "30   30        hora_chegada_prevista       TIME    False       None  False\n",
      "31   31             dia_chegada_real       DATE    False       None  False\n",
      "32   32            hora_chegada_real       TIME    False       None  False\n",
      "33   33               status_partida    VARCHAR    False       None  False\n",
      "34   34               status_chegada    VARCHAR    False       None  False\n",
      "\n",
      "Number of rows: 24288657\n",
      "Number of columns: 35\n"
     ]
    }
   ],
   "source": [
    "# Connect to the DuckDB database\n",
    "con = duckdb.connect(\"flight_monolith.duckdb\")\n",
    "\n",
    "# Retrieve schema information using PRAGMA table_info, which includes:\n",
    "# cid, name, type, notnull, dflt_value, and pk for each column\n",
    "schema_df = con.execute(\"PRAGMA table_info('flights');\").fetchdf()\n",
    "\n",
    "print(\"Schema of the 'flights' table:\")\n",
    "print(schema_df)\n",
    "\n",
    "# Count the number of rows in the flights table\n",
    "row_count = con.execute(\"SELECT COUNT(*) FROM flights;\").fetchone()[0]\n",
    "\n",
    "# The number of columns is the number of rows in the schema_df DataFrame\n",
    "num_columns = schema_df.shape[0]\n",
    "\n",
    "print(f\"\\nNumber of rows: {row_count}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
